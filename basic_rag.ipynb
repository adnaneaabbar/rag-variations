{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1d952a-3f5b-4d89-91c8-a8be5c28daa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0637a909-6de8-41bf-9cf0-12558efef09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED          \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    About an hour ago    \n",
      "llama3.2-vision:latest     085a1fdae525    7.9 GB    2 days ago           \n",
      "phi3:latest                4f2222927938    2.2 GB    3 days ago           \n",
      "gemma2:2b                  8ccf136fdd52    1.6 GB    3 days ago           \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    3 days ago           \n",
      "phi3:medium                cf611a26b048    7.9 GB    3 days ago           \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    3 days ago           \n",
      "mistral:latest             f974a74358d6    4.1 GB    3 days ago           \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43b346e3-31db-4a91-8c05-778b80dfe8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"llama3.2\"\n",
    "\n",
    "llm = ChatOllama(model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7f8044-bf38-438c-a8e3-37b1b6d70eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1664b089-821b-4ee4-8a5f-85b8775b1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34240327-b188-46f0-9e9c-1661df1c504c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collections = chroma_client.list_collections()\n",
    "\n",
    "collections\n",
    "\n",
    "# chroma_client = delete_collection(\"langchain\")\n",
    "# chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a8ebcbd-67d1-41d5-8393-5bbdd6d01549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac09b55b-f4d7-4f67-8074-492c841eee2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition involves breaking down a complex task into smaller, more manageable subtasks. This technique uses \"Chain of thought\" prompting to guide large language models (LLMs) to think step-by-step, ultimately decomposing the original complex task for better model performance. By doing so, task decomposition sheds light on how LLMs approach problem solving and provide a clearer interpretation of their reasoning process. \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Model\n",
    "llm = ChatOllama(model=LLM_MODEL)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf58f4a7-7384-4810-a47e-750d65f69824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique where a model is instructed to \"think step by step\" to break down complicated tasks into smaller and simpler steps, as suggested by the Chain of Thought (CoT) prompting technique. This allows the model to utilize more test-time computation and gain insights into its own thinking process.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94d6ec-49be-448d-88bf-750245d82dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
